{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwUwpnJmS2ke"
   },
   "source": [
    "Maintaining Data Privacy in Association Rule Mining using MASK algorithm\n",
    "\n",
    "By\n",
    "\n",
    "Kozy-Korpesh Tolep, matricola: 5302354  \n",
    "\n",
    "Salah Ismail , matricola: 5239380\n",
    "\n",
    "Amir Mashmool, matricola: 5245307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUhCE2VAS2kg"
   },
   "source": [
    "## Background\n",
    "\n",
    "This paper discusses the challenge of obtaining accurate input data for data mining services while addressing privacy concerns. It explores whether users can be motivated to provide correct information by guaranteeing privacy protection during the mining process. proposing a scheme that distorts user data probabilistically, ensuring a high level of privacy while maintaining accurate mining results. It is this distorted information that is eventually supplied to the data miner, along with a description of the distortion procedure. The performance of the scheme is validated using real and synthetic datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oy2obk5jS2kh"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The paper assumes a database model where each customer's data is represented as a tuple consisting of a fixed-length sequence of 1's and 0's. This model is commonly used for market-basket databases, where columns represent items sold by a supermarket, and each row represents a customer's purchases (1 indicating a purchase and 0 indicating no purchase).\n",
    "\n",
    "The assumption is that the number of 1's (purchases) in the database is significantly smaller than the number of 0's (non-purchases) ==> In short, the database is modeled as a large disk-resident two-dimensional sparse boolean matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWhw6yyzS2kh"
   },
   "source": [
    "## Mining Objectives\n",
    "\n",
    "The mining objective is to efficiently discover all frequent itemsets in the database, which correspond to statistically significant and strong association rules. These rules have a support factor indicating their frequency and a confidence factor representing their strength. The goal is to find interesting rules that surpass user-defined thresholds for support and confidence. A rule is said to be “interesting” if its support and confidence are greater than user-defined thresh-olds $sup_{min}$ and $con_{min}$, respectively.\n",
    "\n",
    "example of a association rule:\n",
    "\n",
    "Transaction 1: {Bread, Milk}\n",
    "Transaction 2: {Bread, Milk, Diapers}\n",
    "\n",
    "Using these measures, we can discover association rules:\n",
    "\n",
    "\n",
    "Bread → Milk (Support: 40%, Confidence: 66.6%)\n",
    "   This rule indicates that if a customer buys bread, they are likely to buy milk as well.\n",
    "\n",
    "   \n",
    "Bread, Milk → Diapers (Support: 40%, Confidence: 50%)\n",
    "    This rule states that if a customer buys both bread and milk, they are likely to buy diapers as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxraZFPHS2ki"
   },
   "source": [
    "## The privacy metric\n",
    "the mechanism adopted in this paper for achieving privacy is to distort the user data before  it is subject to the mining process.\n",
    "As stated in the paper the metric is “with what probability can a given 1 or 0 in the true matrix be reconstructed”.\n",
    "It measures the probability of reconstructing distorted user data. It focuses on individual entries within customer tuples. For many applications, customers may prioritize more privacy for their 1's (purchased actions) compared to their 0's (non-purchased options)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx2UaxkSS2ki"
   },
   "source": [
    "## Quantifying MASK’s Privacy\n",
    "\n",
    "In this section, we present the distortion procedure used by the MASK scheme and quantify the privacy provided by the procedure, as per the above privacy metric.\n",
    "\n",
    "### Distortion Procedure\n",
    "\n",
    "A customer tuple can be considered to be a random vector $ X = \\{X_i\\}$ , such that $X_i = 0 \\ or \\ 1$ .\n",
    "We generate the distorted vector from this customer tuple by computing $Y = distort(X)$ where $Y_i=X_i \\ XOR \\ \\overline r_i$ and $\\overline r_i$ is the complement of $r_i$, a random variable with a density function $f(r) = bernoulli(p) \\ (0 \\leq p \\leq 1)$. That is, $r_i$ takes a value 1 with probability $p$ and 0 with probability $1 - p$.\n",
    "\n",
    "The net effect of the above computation is that the identity of the $i^{th}$ element in $X$ is kept the same with probability $p$ and is flipped with probability $(1 — p)$. All the customer tuples are distorted in this fashion and make up the database supplied to the miner in effect, the miner receives a $probabilistic \\ function$ of the true customer database.\n",
    "\n",
    "### Reconstruction Probability of a 1\n",
    "${R}_1(p)$ is the probability with which a ‘1’ can be reconstructed from the distorted entry as follow:\n",
    "\n",
    "$\\mathcal{R}_1(p)=\\frac{s_0 \\times p^2}{s_0 \\times p+\\left(1-s_0\\right) \\times(1-p)}+\\frac{s_0 \\times(1-p)^2}{s_0 \\times(1-p)+\\left(1-s_0\\right) \\times p}$\n",
    "\n",
    "while $s_0$ is the average support of an item in the database.\n",
    "\n",
    "### The General Reconstruction Equation\n",
    "the relationship between $p$ and the reconstruction probability for the general case where the customer may wish to protect both his $1’s$ and $0’s$, but his concern to keep the $1’s$ private is more than that for the $0’s$.\n",
    "\n",
    "${R}_0(p)$ is the probability with which a ‘0’ can be reconstructed from the distorted entry as follow:\n",
    "\n",
    "$\\mathcal{R}_0(\\mathrm{p})=\\frac{\\left(1-s_0\\right) \\times p^2}{\\left(1-s_0\\right) \\times p+s_0 \\times(1-p)}+\\frac{\\left(1-s_0\\right) \\times(1-p)^2}{s_0 \\times p+\\left(1-s_0\\right) \\times(1-p)}$\n",
    "\n",
    "Our aim is to minimize a weighted average of ${R}_1(p)$ and ${R}_0(p)$. This corresponds to minimizing the probability of reconstruction of both $1’s$ and $0’s$. The $\\textbf {total reconstruction probability}$, ${R}(p)$, is then given as:\n",
    "\n",
    "$\\mathcal{R}(p)=a \\mathcal{R}_1(p)+(1-a) \\mathcal{R}_0(p)$\n",
    "\n",
    "where $a$ is the weight given to $1’s$ over $0’s$.\n",
    "\n",
    "\n",
    "## Privacy Measure\n",
    "Armed with the ability to compute the total reconstruction probability $\\mathcal{R}(p)$, we now simply define $\\textbf {user privacy}$ ${P}(p)$ as the following percentage:\n",
    "\n",
    "$\\mathcal{P}(p)=(1-\\mathcal{R}(p)) * 100$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiOZdID2S2kj"
   },
   "source": [
    "distortion_probability = the distortion probability value given to distortion function\n",
    "\n",
    "distort_dataset() = a function used to distort the dataset based on distortion_probability\n",
    "\n",
    "T = the original true matrix dataset\n",
    "\n",
    "D = the distorted matrix dataset obtained with a distortion probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "wFctNqjgS2kj",
    "outputId": "0740fe90-56f6-454c-dab8-5367f2b60b22"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def distort_dataset(T, distortion_probability):\n",
    "    mask = np.random.random(T.shape) > (1 - distortion_probability)\n",
    "    D = (T + mask) % 2 # %2 is to ensures that all values are either 0 or 1.\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx-fyuDoS2kl"
   },
   "source": [
    "$s_0$ is the average support of an item in the database.\n",
    "\n",
    "$a$ is the weight given to $1’s$ over $0’s$.\n",
    "\n",
    "${P}(p)$ is user privacy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTJlpGRIS2kl"
   },
   "source": [
    "###### Computing ${R}_1(p)$ and ${R}_0(p)$\n",
    "###### Computing the total reconstruction probability ${R}(p)$\n",
    "###### Computing the privacy metric ${P}(p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "B9tILxs_S2kl",
    "outputId": "256badff-4ccf-4174-ddef-1bdf38b9a546"
   },
   "outputs": [],
   "source": [
    "def user_privacy(s_0, a, p):\n",
    "    r1_p = ((s_0 * (p ** 2)) / ((s_0 * p) + ((1 - s_0) * (1 - p)))) + ((s_0 * ((1 - p) ** 2)) / ((s_0 * (1 - p)) + ((1 - s_0) * p)))\n",
    "    r0_p = (((1 - s_0) * (p ** 2)) / (((1 - s_0) * p) + (s_0 * (1 - p)))) + ((s_0 * ((1 - p) ** 2)) / ((s_0 * p) + ((1 - s_0) * (1 - p))))\n",
    "\n",
    "    r_p = (a * r1_p) + ((1 - a) * r0_p)\n",
    "\n",
    "    p_p = (1 - r_p) * 100\n",
    "\n",
    "    return p_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6Ifjkp7S2km"
   },
   "source": [
    "## Mining the Distorted Database (MASK Algorithm)\n",
    "\n",
    "MASK’s technique is for estimating the true (accurate) supports of itemsets from a distorted database.\n",
    "We first show how to estimate the supports of $1-itemsets$ (i.e. singletons) and then present the general $n-itemset$ support estimation procedure.\n",
    "\n",
    "It is important to keep in mind that the miner is provided with both the distorted matrix as well as the distortion probability, that is, it knows the value of $p$ that was used in distorting the true matrix.\n",
    "\n",
    "### Estimating $1-itemsets$ Supports\n",
    "Now we have two matrices, first is denoted as $T$ which is original true matrix and second one is the distorted matrix, obtained with a distortion probability of $p$, denoted as $D$.\n",
    "\n",
    "Lets find out formula for finding support of a 1-itemset. Lets take any column $i$ of $T$, it has some amount of $1’s$ and $0’s$ which are $c_1^T$ and $c_0^T$, while same column $i$ has values $c_1^D$ and $c_0^D$ in the matrix $D$.\n",
    "With this notation, we calculate the support of $i$ in $T$ using the equation below:\n",
    "\n",
    "$\\mathbf{C}^T=\\mathbf{M}^{-1} \\mathbf{C}^{\\mathrm{D}}$\n",
    "\n",
    "Where\n",
    "\n",
    "$M=\\left[\\begin{array}{cc}p & 1-p \\\\ 1-p & p\\end{array}\\right] \\  C^D=\\left[\\begin{array}{c}c_1^D \\\\ c_0^D\\end{array}\\right] C^T=\\left[\\begin{array}{c}c_1^T \\\\ c_0^T\\end{array}\\right]$\n",
    "\n",
    "The M matrix in the above equation incorporates the observation that by our method of distortion, if a column had $n \\ 1’s$ in $T$, these $1’s$ will generate approximately $pn \\ 1’s$ and $(1 — p)n \\ 0’s$ for the same column in $D$. Similarly for the $0’s$ of this column in $T$. Therefore, given $c_1^D$ and $c_0^D$, it is possible to estimate the value of $c_1^T$, that is, the true support of item $i$.\n",
    "\n",
    "\n",
    "### Estimating $n-itemsets$ Supports\n",
    "If we extend formula for 1-itemsets we can compute the support for an arbitrary n-itemset. For this general case, we define the matrices as:\n",
    "\n",
    "$C^D=\\left[\\begin{array}{c}c_{2^n-1}^D \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ c_1^D \\\\ c_0^D\\end{array}\\right] \\quad \\ C^T=\\left[\\begin{array}{c}c_{2^n-1}^T \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ c_1^T \\\\ c_0^T\\end{array}\\right]$\n",
    "\n",
    "Here $c_k^T$ should be interpreted as the count of the tuples in $T$ that have the binary form of $k$ (in $n$ digits) for the given itemset (that is, for a 2 -itemset, $c_2^T$ refers to the count of 10's in the columns of $T$ corresponding to that itemset, $c_3^T$ to the count of 11 's, and so on). Similarly, $c_k^D$ is defined for the distorted matrix $D$. Finally, the matrix $\\mathbf{M}$ is defined as:\n",
    "\n",
    "$m_{i, j}=$ The probability that a tuple of the form corresponding to $c_j^T$ in $T$ goes to a tuple of the form corresponding to $c_i^D$ in $D$.\n",
    "\n",
    "For example, $m_{1, 2}$ for a $2-itemset$ is the probability that a $10$ tuple distorts to a $01$ tuple. Accordingly, $m_{1, 2}=$ $(1-p)(1-p)$. The basis for this formulation lies in the fact that in our distortion procedure, the component columns of an $n-itemset$ are distorted $idependently$. Therefore, we can use the product of the probability terms. As a result matrix $\\mathbf{M}$ for a $2-itemset$ is equal to:\n",
    "$M=\\left[\\begin{array}{cc}p^2 & p(1-p) & p(1-p) & (1-p)^2 \\\\ p(1-p) & p^2 & (1-p)^2 & p(1-p) \\\\ p(1-p) & (1-p)^2 & p^2 & p(1-p) \\\\ (1-p)^2 & p(1-p) & p(1-p) &p^2  \\end{array}\\right]$\n",
    "\n",
    "### The Full Mining Process\n",
    "After we found $c_i^D$, we can find $c_{2^n-1}^T$ for an $n-itemset$. However, first we need to find $c_i^D$ values themselves. For this purpose, there was proposed implementation of the system which is based on the classical Apriori algorithm.\n",
    "\n",
    "The Apriori algorithm helps to find out frequent itemsets by gradually increasing number of the itemsets. It uses algorithm called AprioriGen to create candidate itemsets for every pass by using previos frequent itemsets. With help of this algrorithm we can efficiently search frequent itemset in large datasets, by looking for a subsets that likely to be frequent.\n",
    "\n",
    "Algorithm proposed in the paper, named MASK is based on Apriori, but it has critical difference. For instance, lets consider we are countring 2-itemsets. Apriori will count only one binary form $'11'$ for each tuple in the candidate 2-itemset, which means that both items in the subset has positive value in the dataset. However, Mask goes further and counts different combinations of the 2-itemsets: $'00', '01', '10'$, and $'11'$. Thus, we can keep track of the co-occurrences too. Another important difference is that true support is calculated after each pass, not tuple-by-tuple basis. Furthermore, if the same value of p is used for all columns matrix M is same for all candidates and needs to be calculate only once at the end of the pass. Finally, the size of the Matrix is $O(2^n)$ as it depends on n, of the n-itemset.\n",
    "\n",
    "## MASK Mining Optimizations\n",
    "\n",
    "### Linear Number of Counters\n",
    "First, we need to focus on reducing number of counters. As it can be quite compute-intensive to find square matrix $O(2^n)$ for each itemset. If we closely look on how true support is calculated, we can find some ways for optimization. M matrix is inverted and multiplied to all combination of the n-itemset. As a result, reconstructed support is the weighted sum of the counts of different combinations of $2^n$ componentes of the distorted database. However, we can denote that there are only n+1 distinct weights for $2^n$ weights. For instance, for a 2-itemset estimated reconstructed support can be found by formula:\n",
    "\n",
    "$s_{est}=a_{1}C_{00}^{D}+a_{1}C_{01}^{D}+a_{1}C_{02}^{D}+a_{1}C_{03}^{D}$\n",
    "\n",
    "where $C_{00}^{D}$ is the count of xy tuples in the distorted database and $a_{i}$ are the associated weights. Here, weights of $a_2$ and $a_3$ are equal because probability that $'11'$ distorts to $'10'$ is equal to the probability that $'11'$ distorts to $'01'$ both are equal to $p(1-p)$. Thus, reverse component weights are also equal. It means that we need to calculate weights of only three combinations $'00', '11', '01'$ or $'10'$. The above observation can be generalized to an n-itemset. Number of counters can be reduced from $2^n$ to $n+1$. As a result counters of items that has unique number of $1$'s will have distinct number, for example, for a 3-itemset we need to find counter of 4 combinations: one for $'000'$ and $'111'$, one for one occurence of '1' in the triplet $(001, 010, 100)$ and one for two occurences of $'1'$ $(011, 110, 101)$.   \n",
    "### Reducing Amount of Counting\n",
    "We can also make reduction in amount of counting with help of simple algebraic properties. Based on the fact that $C_{00} + C_{01} + C_{10} + C_{11}$ must be equal to the cardinality of the database, we can choose to to count of these components.\n",
    "\n",
    "Counting of only $'11'$ in the 2-itemset will take $O(tlen^2)$ operations, where tlen is the transaction length. It can be reduced using following technique. Initially, for a item-list, we need to keep track of all identifiers that have 1's in the transaction, from this list we remove all singleton itemset identifiers that were calculated in the previos pass. The next stage is to create complement-list, which consists of previousy estimated all frequent 1-itemsets that do not appear in the current transaction. Let the item-list and complement-list be of lenght $m_1$ and $m_2$, so $|m_1+m_2|=F_1$, where $F_1$ is total number of frequent 1-itemsets. If we now, use this technique to calculate 11's, 01's and 10's, it will take $O(m_1^2)+O(m_1m_2)$ operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "Ki_KHXXKS2km",
    "outputId": "582bcd44-3020-4b13-bb6f-bebe96d5f021"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import string\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def MASK(dataset, dist_P, min_support, verbose=False):\n",
    "    all_frequent_itemsets = [] #frequent itemset of all passes\n",
    "    all_support_values = [] #estimated support for each n-itemset\n",
    "\n",
    "    columns_list, columns_index_map = generate_items_and_column_map(dataset)\n",
    "\n",
    "    passes = dataset.shape[1] #number of passes\n",
    "    start_time = time.time()\n",
    "\n",
    "    for p in range(passes):\n",
    "        n = p + 1 #each pass generate n-itemset tuple (path = 0, 1-itemset and etc.)\n",
    "        print(\"\\nPASS: \", n, \"( \",n, \"-itemsets)\")\n",
    "        column_name_combinations = generate_column_name_combinations(columns_list, n) #generating all possible combinations of column names for each n-itemset\n",
    "        if verbose:\n",
    "          print(\"column_name_combinations\",column_name_combinations)\n",
    "        #Optimization N2 (Reducing amount of counting)\n",
    "        if n>1:\n",
    "          #frequent_itemsets array to keep track n-combination with support LARGER than min_support\n",
    "          #non_frequent_n_itemsets array to keep track n-combination with support SMALLER than min_support\n",
    "          column_name_combinations = filter_non_frequent_n_itemsets(column_name_combinations, non_frequent_n_itemsets)\n",
    "        #we don't need to calculate number of 0's as it can be found by subtracting number of 1's from number of transactions\n",
    "        possible_ones = np.array(range(1, n + 1)) #quantity of ones that can be in the combination depends of number of n - n=1 possible ones = [1], n=2 possible ones = [1,2]...\n",
    "        weights = compute_weights(dist_P, n, possible_ones) #weights is array which is computed from formula (#) weights = [weight corresoponding to each possible value of one in the tuple]\n",
    "\n",
    "        frequent_n_itemsets, support_n_itemsets, non_frequent_n_itemsets = compute_support(n, dataset, columns_index_map, column_name_combinations,\n",
    "                                                                  weights, dist_P, min_support)\n",
    "\n",
    "        all_frequent_itemsets.extend(frequent_n_itemsets)\n",
    "        all_support_values.extend(support_n_itemsets)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Frequent itemsets for this pass are: \", frequent_n_itemsets)\n",
    "            print(\"The estimated support values of them are: \", support_n_itemsets)\n",
    "            print(\"Non-frequent itemsets that should not be part of the combinations of the next pass are: \",\n",
    "                  non_frequent_n_itemsets)\n",
    "\n",
    "        if np.size(frequent_n_itemsets) == 0:\n",
    "            break\n",
    "\n",
    "    print(\"The algorithm stopped because there are no more frequent itemsets!\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_time = str(round(execution_time, 2))\n",
    "    if verbose:\n",
    "        print(\"Processing time is:\", execution_time, \"sec.\")\n",
    "\n",
    "    return np.array(all_frequent_itemsets), np.array(all_support_values)\n",
    "\n",
    "\n",
    "def generate_items_and_column_map(dataset):\n",
    "    alphabet = string.ascii_uppercase\n",
    "    columns_list = list(alphabet)[:dataset.shape[1]] #converting column names to list of Alphabet strings [A, B, C, D ...]\n",
    "    columns_index_map = {item: column_index for column_index, item in enumerate(columns_list)} #creating set where key is the string from columns_list and value is their index {'A':0, 'B':1,'C':2, ...}\n",
    "    return columns_list, columns_index_map\n",
    "\n",
    "\n",
    "def generate_column_name_combinations(items, n):\n",
    "    column_name_combinations = [''.join(c) for c in combinations(items, n)]\n",
    "    return np.array(column_name_combinations)\n",
    "\n",
    "\n",
    "def filter_non_frequent_n_itemsets(column_name_combinations, non_frequent_n_itemsets):\n",
    "    #non_freq_singletons = ['A', 'B',...] non frequent singletons from non_frequent_n_itemsets\n",
    "    non_freq_singletons = list(set([c for comb in non_frequent_n_itemsets for c in comb]))\n",
    "    #new column_name_combinations without non_freq_singletons\n",
    "    column_name_combinations = [comb for comb in column_name_combinations if not any(c in comb for c in non_freq_singletons)]\n",
    "    return np.array(column_name_combinations)\n",
    "\n",
    "\n",
    "def compute_weights(dist_P, n, possible_ones):\n",
    "    #according to formula calculating weights corresponding to possible number of ones\n",
    "    weights = (dist_P ** (n - possible_ones)) * ((1 - dist_P) ** possible_ones)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def compute_support(n, dataset, item_column_map, column_name_combinations, weights, dist_P, min_support):\n",
    "    frequent_n_itemsets = []\n",
    "    support_n_itemsets = []\n",
    "    non_frequent_n_itemsets = []\n",
    "\n",
    "    for column_name_combination in column_name_combinations:\n",
    "        columns = [item_column_map[item] for item in column_name_combination]\n",
    "        # print(\"columns inside compute support\", columns)\n",
    "        column_values = dataset[:, columns]\n",
    "        # print(\"column_values\", column_values)\n",
    "        list_column_sum = column_values.sum(axis=1)\n",
    "        # print(\"list_column_sum\",list_column_sum)\n",
    "        # Optimization N2 we have calculated weights for each number of possible \"1\"s now we need to calculate number of \"1\"s corresponding to that weight\n",
    "        number_of_ones = np.array([np.count_nonzero(list_column_sum == i + 1) for i in range(n)])\n",
    "        number_of_zeros = dataset.shape[0] - np.sum(number_of_ones) #according to optimization N2, we don't need to count number of tuples with 0 only just need to subtract number of cols - cols with 1\n",
    "\n",
    "        weight_row_zeros = dist_P ** n #weight of tuple consisting only zeros\n",
    "        estimated_support = (weight_row_zeros * number_of_zeros) + np.sum(weights * number_of_ones) #estimated support according to formula #\n",
    "\n",
    "        if estimated_support >= min_support:\n",
    "            frequent_n_itemsets.append(column_name_combination)\n",
    "            support_n_itemsets.append(estimated_support)\n",
    "        else:\n",
    "            non_frequent_n_itemsets.append(column_name_combination)\n",
    "\n",
    "    return np.array(frequent_n_itemsets), np.array(support_n_itemsets), np.array(non_frequent_n_itemsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p2jyjAxS2ko"
   },
   "source": [
    "## Performance Framework\n",
    "Due to the probabilistic nature of MASK, the reconstructed support values are not expected to match the actual supports exactly. This can lead to errors where the reported support values are either larger or smaller than the actual supports.\n",
    "\n",
    "Errors in support estimation can have a significant impact, the probabilistic evaluation of supports may incorrectly classify \"border-line\" itemsets as either frequent or rare. This results in both false positives (itemsets wrongly reported as frequent) and false negatives (itemsets wrongly reported as rare).\n",
    "\n",
    "We evaluate the mining process under two conditions. The first condition uses the $sup_{min}$ value provided by the user, while the second condition uses a slightly lower $sup_{min}$ value.\n",
    "\n",
    "We prioritize coverage over precision in this evaluation. Specifically, we consider a 10% reduction in the $sup_{min}$ value (r = 10%).\n",
    "This means lowering the threshold for what is considered significant, allowing for greater inclusion of items but potentially reducing precision or increasing false positives.\n",
    "\n",
    "To quantify the errors made, We compare the mining outputs obtained from MASK with those derived from Apriori running on the true (undistorted) database. We compare the results using both the $sup_{min}$ value provided by the user and the lowered $sup_{min}$ value. This allows to evaluate the impact of the lowered $sup_{min}$ value on the mining process and assess the differences between MASK and Apriori.\n",
    "\n",
    "### Error Metrics\n",
    "\n",
    "1.Support Error  $\\rho$.\n",
    "\n",
    "This metric reflects the (percentage) average relative error in the reconstructed support values for those itemsets that are correctly  identified to be frequent. Denoting the reconstructed support by rec_sup and the actual support by act_sup, the support error is computed over all frequent itemsets as\n",
    "\n",
    "$\\rho=\\frac{1}{|f|} \\Sigma_f \\frac{\\left|r e c_{-} s u p_f-a c t_{-} s u p_f\\right|}{a c t_{\\_} s u p_f} * 100$\n",
    "\n",
    "2.Identity Error.\n",
    "\n",
    "This metric reflects the (percentage) error in identifying frequent itemsets and has two components: $\\sigma^{+}$, indicating the percentage of false positives, and $\\sigma^{-}$ indicating the percentage of false negatives. Denoting the reconstructed set of frequent itemsets with $R$ and the correct set of frequent itemsets with $F$, these metrics are computed as:\n",
    "\n",
    "$\\sigma^{+}=\\frac{|R-F|}{|F|} * 100 \\quad \\sigma^{-}=\\frac{|F-R|}{|F|} * 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "T2jfoBTOS2ko",
    "outputId": "e0cf6c99-e004-4926-dc44-94a93a8c79b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_support_error(mask_frequent, mask_f_support, apriori_f_support):\n",
    "    \"\"\"\n",
    "    Calculates the support error metric.\n",
    "\n",
    "    Args:\n",
    "    mask_frequent (list): List of frequent itemsets obtained from MASK.\n",
    "    mask_f_support (list): List of estimated supports of frequent itemsets from MASK.\n",
    "    apriori_f_support (list): List of actual supports of frequent itemsets from Apriori.\n",
    "\n",
    "    Returns:\n",
    "    float: Support error.\n",
    "    \"\"\"\n",
    "    total_itemsets = len(mask_frequent)\n",
    "\n",
    "    # Calculate the relative support error for each itemset and sum them up\n",
    "    relative_errors = [abs(rec_sup - act_sup) / act_sup for rec_sup, act_sup in zip(mask_f_support, apriori_f_support)]\n",
    "    sum_of_errors = sum(relative_errors)\n",
    "\n",
    "    # Calculate the average relative support error\n",
    "    average_error = sum_of_errors / total_itemsets\n",
    "\n",
    "    support_error = average_error\n",
    "    return support_error\n",
    "\n",
    "def calculate_identity_error(mask_frequent, apriori_frequent):\n",
    "    \"\"\"\n",
    "    Calculates the identity error metrics.\n",
    "\n",
    "    Args:\n",
    "    mask_frequent (list): List of frequent itemsets obtained from MASK.\n",
    "    apriori_frequent (list): List of frequent itemsets obtained from Apriori.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Identity error (false positives, false negatives).\n",
    "    \"\"\"\n",
    "    reconstructed_itemsets = set(mask_frequent)\n",
    "    print(apriori_frequent)\n",
    "    actual_itemsets = set(apriori_frequent)\n",
    "    false_positives = len(reconstructed_itemsets - actual_itemsets) / len(actual_itemsets)\n",
    "    false_negatives = len(actual_itemsets - reconstructed_itemsets) / len(actual_itemsets)\n",
    "    return false_positives, false_negatives\n",
    "\n",
    "def support_identity_errors(mask_frequent, mask_f_support, apriori_frequent, apriori_f_support):\n",
    "    nbr_m_f = np.size(mask_frequent)\n",
    "    max_levels = len(mask_frequent[nbr_m_f-1])\n",
    "\n",
    "    levels = []\n",
    "    support_errors = []\n",
    "    false_positive_errors = []\n",
    "    false_negative_errors = []\n",
    "\n",
    "    for l in range(max_levels):\n",
    "        level = l+1\n",
    "        level_mask_frequent = []\n",
    "        level_mask_f_support = []\n",
    "        for i in range(np.size(mask_frequent)):\n",
    "            if len(mask_frequent[i]) == level:\n",
    "                level_mask_frequent.append(mask_frequent[i])\n",
    "                level_mask_f_support.append(mask_f_support[i])\n",
    "\n",
    "        support_error = calculate_support_error(level_mask_frequent, level_mask_f_support, apriori_f_support)\n",
    "\n",
    "        false_positive, false_negative = calculate_identity_error(level_mask_frequent, apriori_frequent)\n",
    "\n",
    "        levels.append(level)\n",
    "        support_errors.append(support_error)\n",
    "        false_positive_errors.append(false_positive)\n",
    "        false_negative_errors.append(false_negative)\n",
    "\n",
    "    return levels, support_errors, false_positive_errors, false_negative_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMlskf8FS2kp"
   },
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e596pZx_S2kp"
   },
   "source": [
    "\n",
    "### Applying The MASK on a synthetic dataset\n",
    "\n",
    "#### Distortion Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "vdxLYS3_S2kp",
    "outputId": "5a5b789d-1f92-4289-ab1d-54a1b2303239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True matrix:  [[1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]]\n",
      "Distorted matrix:  [[1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]]\n",
      "User Privacy attained:  71.86866799534961 %\n"
     ]
    }
   ],
   "source": [
    "# creating True matrix T_dataset\n",
    "T_dataset = np.array([\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
    "[0, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0],\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
    "[0, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0],\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1],\n",
    "[0, 1, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0],\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
    "])\n",
    "\n",
    "distortion_probability = 0.9\n",
    "\n",
    "# building the distorted matrix D_dataset by distorting T_dataset\n",
    "# D_dataset = distort_dataset(T_dataset, distortion_probability)\n",
    "D_dataset = T_dataset\n",
    "\n",
    "s_0 = 0.01\n",
    "a = 0.75\n",
    "privacy = user_privacy(s_0, a, distortion_probability)\n",
    "print(\"True matrix: \", T_dataset)\n",
    "print(\"Distorted matrix: \", D_dataset)\n",
    "print(\"User Privacy attained: \", privacy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNqJ9xk3S2kp"
   },
   "source": [
    "#### Mining Procedure\n",
    "\n",
    "MASK() will provide us with\n",
    "1. mask_frequent which is a list of all frequent itemsets and\n",
    "2. mask_f_support which is a list of their corresponding estimated supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "QEJMgBJJS2kp",
    "outputId": "0b6fb77d-d1e4-4847-a2dc-a26d2774dd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PASS:  1 (  1 -itemsets)\n",
      "column_name_combinations ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "Frequent itemsets for this pass are:  ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "The estimated support values of them are:  [3.4 6.6 5.8 5.8 4.2 1.  6.6 9.  3.4 5.8]\n",
      "Non-frequent itemsets that should not be part of the combinations of the next pass are:  []\n",
      "\n",
      "PASS:  2 (  2 -itemsets)\n",
      "column_name_combinations ['AB' 'AC' 'AD' 'AE' 'AF' 'AG' 'AH' 'AI' 'AJ' 'BC' 'BD' 'BE' 'BF' 'BG'\n",
      " 'BH' 'BI' 'BJ' 'CD' 'CE' 'CF' 'CG' 'CH' 'CI' 'CJ' 'DE' 'DF' 'DG' 'DH'\n",
      " 'DI' 'DJ' 'EF' 'EG' 'EH' 'EI' 'EJ' 'FG' 'FH' 'FI' 'FJ' 'GH' 'GI' 'GJ'\n",
      " 'HI' 'HJ' 'IJ']\n",
      "Frequent itemsets for this pass are:  ['AB' 'AC' 'AD' 'AE' 'AF' 'AG' 'AH' 'AI' 'AJ' 'BC' 'BD' 'BE' 'BF' 'BG'\n",
      " 'BH' 'BI' 'BJ' 'CD' 'CE' 'CF' 'CG' 'CH' 'CI' 'CJ' 'DE' 'DF' 'DG' 'DH'\n",
      " 'DI' 'DJ' 'EF' 'EG' 'EH' 'EI' 'EJ' 'FG' 'FH' 'FI' 'FJ' 'GH' 'GI' 'GJ'\n",
      " 'HI' 'HJ' 'IJ']\n",
      "The estimated support values of them are:  [0.9  2.74 2.74 0.66 0.34 2.82 3.06 2.5  2.74 3.06 3.06 3.54 0.66 3.78\n",
      " 5.94 0.9  3.06 4.9  0.9  0.58 3.06 5.22 2.74 4.9  0.9  0.58 3.06 5.22\n",
      " 2.74 4.9  0.42 3.54 3.78 0.66 0.9  0.66 0.9  0.34 0.58 5.94 2.82 3.06\n",
      " 3.06 5.22 2.74]\n",
      "Non-frequent itemsets that should not be part of the combinations of the next pass are:  []\n",
      "\n",
      "PASS:  3 (  3 -itemsets)\n",
      "column_name_combinations ['ABC' 'ABD' 'ABE' 'ABF' 'ABG' 'ABH' 'ABI' 'ABJ' 'ACD' 'ACE' 'ACF' 'ACG'\n",
      " 'ACH' 'ACI' 'ACJ' 'ADE' 'ADF' 'ADG' 'ADH' 'ADI' 'ADJ' 'AEF' 'AEG' 'AEH'\n",
      " 'AEI' 'AEJ' 'AFG' 'AFH' 'AFI' 'AFJ' 'AGH' 'AGI' 'AGJ' 'AHI' 'AHJ' 'AIJ'\n",
      " 'BCD' 'BCE' 'BCF' 'BCG' 'BCH' 'BCI' 'BCJ' 'BDE' 'BDF' 'BDG' 'BDH' 'BDI'\n",
      " 'BDJ' 'BEF' 'BEG' 'BEH' 'BEI' 'BEJ' 'BFG' 'BFH' 'BFI' 'BFJ' 'BGH' 'BGI'\n",
      " 'BGJ' 'BHI' 'BHJ' 'BIJ' 'CDE' 'CDF' 'CDG' 'CDH' 'CDI' 'CDJ' 'CEF' 'CEG'\n",
      " 'CEH' 'CEI' 'CEJ' 'CFG' 'CFH' 'CFI' 'CFJ' 'CGH' 'CGI' 'CGJ' 'CHI' 'CHJ'\n",
      " 'CIJ' 'DEF' 'DEG' 'DEH' 'DEI' 'DEJ' 'DFG' 'DFH' 'DFI' 'DFJ' 'DGH' 'DGI'\n",
      " 'DGJ' 'DHI' 'DHJ' 'DIJ' 'EFG' 'EFH' 'EFI' 'EFJ' 'EGH' 'EGI' 'EGJ' 'EHI'\n",
      " 'EHJ' 'EIJ' 'FGH' 'FGI' 'FGJ' 'FHI' 'FHJ' 'FIJ' 'GHI' 'GHJ' 'GIJ' 'HIJ']\n",
      "Frequent itemsets for this pass are:  ['ABC' 'ABD' 'ABE' 'ABG' 'ABH' 'ABI' 'ABJ' 'ACD' 'ACE' 'ACF' 'ACG' 'ACH'\n",
      " 'ACI' 'ACJ' 'ADE' 'ADF' 'ADG' 'ADH' 'ADI' 'ADJ' 'AEG' 'AEH' 'AEI' 'AEJ'\n",
      " 'AFG' 'AFH' 'AFJ' 'AGH' 'AGI' 'AGJ' 'AHI' 'AHJ' 'AIJ' 'BCD' 'BCE' 'BCF'\n",
      " 'BCG' 'BCH' 'BCI' 'BCJ' 'BDE' 'BDF' 'BDG' 'BDH' 'BDI' 'BDJ' 'BEF' 'BEG'\n",
      " 'BEH' 'BEI' 'BEJ' 'BFG' 'BFH' 'BFJ' 'BGH' 'BGI' 'BGJ' 'BHI' 'BHJ' 'BIJ'\n",
      " 'CDE' 'CDF' 'CDG' 'CDH' 'CDI' 'CDJ' 'CEG' 'CEH' 'CEI' 'CEJ' 'CFG' 'CFH'\n",
      " 'CFI' 'CFJ' 'CGH' 'CGI' 'CGJ' 'CHI' 'CHJ' 'CIJ' 'DEG' 'DEH' 'DEI' 'DEJ'\n",
      " 'DFG' 'DFH' 'DFI' 'DFJ' 'DGH' 'DGI' 'DGJ' 'DHI' 'DHJ' 'DIJ' 'EFG' 'EFH'\n",
      " 'EGH' 'EGI' 'EGJ' 'EHI' 'EHJ' 'EIJ' 'FGH' 'FGI' 'FGJ' 'FHI' 'FHJ' 'FIJ'\n",
      " 'GHI' 'GHJ' 'GIJ' 'HIJ']\n",
      "The estimated support values of them are:  [0.522 0.522 0.378 0.594 0.81  0.306 0.522 2.434 0.306 0.274 2.25  2.466\n",
      " 2.218 2.434 0.306 0.274 2.25  2.466 2.218 2.434 0.57  0.594 0.282 0.306\n",
      " 0.282 0.306 0.274 2.538 2.226 2.25  2.25  2.466 2.218 2.466 0.594 0.306\n",
      " 0.81  2.754 0.522 2.466 0.594 0.306 0.81  2.754 0.522 2.466 0.354 2.97\n",
      " 3.186 0.378 0.594 0.378 0.594 0.306 3.402 0.594 0.81  0.81  2.754 0.522\n",
      " 0.522 0.49  2.466 4.41  2.434 4.378 0.594 0.81  0.306 0.522 0.306 0.522\n",
      " 0.274 0.49  2.754 2.25  2.466 2.466 4.41  2.434 0.594 0.81  0.306 0.522\n",
      " 0.306 0.522 0.274 0.49  2.754 2.25  2.466 2.466 4.41  2.434 0.354 0.378\n",
      " 3.186 0.57  0.594 0.594 0.81  0.306 0.594 0.282 0.306 0.306 0.522 0.274\n",
      " 2.538 2.754 2.25  2.466]\n",
      "Non-frequent itemsets that should not be part of the combinations of the next pass are:  ['ABF' 'AEF' 'AFI' 'BFI' 'CEF' 'DEF' 'EFI' 'EFJ']\n",
      "\n",
      "PASS:  4 (  4 -itemsets)\n",
      "column_name_combinations ['ABCD' 'ABCE' 'ABCF' 'ABCG' 'ABCH' 'ABCI' 'ABCJ' 'ABDE' 'ABDF' 'ABDG'\n",
      " 'ABDH' 'ABDI' 'ABDJ' 'ABEF' 'ABEG' 'ABEH' 'ABEI' 'ABEJ' 'ABFG' 'ABFH'\n",
      " 'ABFI' 'ABFJ' 'ABGH' 'ABGI' 'ABGJ' 'ABHI' 'ABHJ' 'ABIJ' 'ACDE' 'ACDF'\n",
      " 'ACDG' 'ACDH' 'ACDI' 'ACDJ' 'ACEF' 'ACEG' 'ACEH' 'ACEI' 'ACEJ' 'ACFG'\n",
      " 'ACFH' 'ACFI' 'ACFJ' 'ACGH' 'ACGI' 'ACGJ' 'ACHI' 'ACHJ' 'ACIJ' 'ADEF'\n",
      " 'ADEG' 'ADEH' 'ADEI' 'ADEJ' 'ADFG' 'ADFH' 'ADFI' 'ADFJ' 'ADGH' 'ADGI'\n",
      " 'ADGJ' 'ADHI' 'ADHJ' 'ADIJ' 'AEFG' 'AEFH' 'AEFI' 'AEFJ' 'AEGH' 'AEGI'\n",
      " 'AEGJ' 'AEHI' 'AEHJ' 'AEIJ' 'AFGH' 'AFGI' 'AFGJ' 'AFHI' 'AFHJ' 'AFIJ'\n",
      " 'AGHI' 'AGHJ' 'AGIJ' 'AHIJ' 'BCDE' 'BCDF' 'BCDG' 'BCDH' 'BCDI' 'BCDJ'\n",
      " 'BCEF' 'BCEG' 'BCEH' 'BCEI' 'BCEJ' 'BCFG' 'BCFH' 'BCFI' 'BCFJ' 'BCGH'\n",
      " 'BCGI' 'BCGJ' 'BCHI' 'BCHJ' 'BCIJ' 'BDEF' 'BDEG' 'BDEH' 'BDEI' 'BDEJ'\n",
      " 'BDFG' 'BDFH' 'BDFI' 'BDFJ' 'BDGH' 'BDGI' 'BDGJ' 'BDHI' 'BDHJ' 'BDIJ'\n",
      " 'BEFG' 'BEFH' 'BEFI' 'BEFJ' 'BEGH' 'BEGI' 'BEGJ' 'BEHI' 'BEHJ' 'BEIJ'\n",
      " 'BFGH' 'BFGI' 'BFGJ' 'BFHI' 'BFHJ' 'BFIJ' 'BGHI' 'BGHJ' 'BGIJ' 'BHIJ'\n",
      " 'CDEF' 'CDEG' 'CDEH' 'CDEI' 'CDEJ' 'CDFG' 'CDFH' 'CDFI' 'CDFJ' 'CDGH'\n",
      " 'CDGI' 'CDGJ' 'CDHI' 'CDHJ' 'CDIJ' 'CEFG' 'CEFH' 'CEFI' 'CEFJ' 'CEGH'\n",
      " 'CEGI' 'CEGJ' 'CEHI' 'CEHJ' 'CEIJ' 'CFGH' 'CFGI' 'CFGJ' 'CFHI' 'CFHJ'\n",
      " 'CFIJ' 'CGHI' 'CGHJ' 'CGIJ' 'CHIJ' 'DEFG' 'DEFH' 'DEFI' 'DEFJ' 'DEGH'\n",
      " 'DEGI' 'DEGJ' 'DEHI' 'DEHJ' 'DEIJ' 'DFGH' 'DFGI' 'DFGJ' 'DFHI' 'DFHJ'\n",
      " 'DFIJ' 'DGHI' 'DGHJ' 'DGIJ' 'DHIJ' 'EFGH' 'EFGI' 'EFGJ' 'EFHI' 'EFHJ'\n",
      " 'EFIJ' 'EGHI' 'EGHJ' 'EGIJ' 'EHIJ' 'FGHI' 'FGHJ' 'FGIJ' 'FHIJ' 'GHIJ']\n",
      "Frequent itemsets for this pass are:  []\n",
      "The estimated support values of them are:  []\n",
      "Non-frequent itemsets that should not be part of the combinations of the next pass are:  []\n",
      "The algorithm stopped because there are no more frequent itemsets!\n",
      "Processing time is: 0.1 sec.\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'AB' 'AC' 'AD' 'AE' 'AF' 'AG'\n",
      " 'AH' 'AI' 'AJ' 'BC' 'BD' 'BE' 'BF' 'BG' 'BH' 'BI' 'BJ' 'CD' 'CE' 'CF'\n",
      " 'CG' 'CH' 'CI' 'CJ' 'DE' 'DF' 'DG' 'DH' 'DI' 'DJ' 'EF' 'EG' 'EH' 'EI'\n",
      " 'EJ' 'FG' 'FH' 'FI' 'FJ' 'GH' 'GI' 'GJ' 'HI' 'HJ' 'IJ' 'ABC' 'ABD' 'ABE'\n",
      " 'ABG' 'ABH' 'ABI' 'ABJ' 'ACD' 'ACE' 'ACF' 'ACG' 'ACH' 'ACI' 'ACJ' 'ADE'\n",
      " 'ADF' 'ADG' 'ADH' 'ADI' 'ADJ' 'AEG' 'AEH' 'AEI' 'AEJ' 'AFG' 'AFH' 'AFJ'\n",
      " 'AGH' 'AGI' 'AGJ' 'AHI' 'AHJ' 'AIJ' 'BCD' 'BCE' 'BCF' 'BCG' 'BCH' 'BCI'\n",
      " 'BCJ' 'BDE' 'BDF' 'BDG' 'BDH' 'BDI' 'BDJ' 'BEF' 'BEG' 'BEH' 'BEI' 'BEJ'\n",
      " 'BFG' 'BFH' 'BFJ' 'BGH' 'BGI' 'BGJ' 'BHI' 'BHJ' 'BIJ' 'CDE' 'CDF' 'CDG'\n",
      " 'CDH' 'CDI' 'CDJ' 'CEG' 'CEH' 'CEI' 'CEJ' 'CFG' 'CFH' 'CFI' 'CFJ' 'CGH'\n",
      " 'CGI' 'CGJ' 'CHI' 'CHJ' 'CIJ' 'DEG' 'DEH' 'DEI' 'DEJ' 'DFG' 'DFH' 'DFI'\n",
      " 'DFJ' 'DGH' 'DGI' 'DGJ' 'DHI' 'DHJ' 'DIJ' 'EFG' 'EFH' 'EGH' 'EGI' 'EGJ'\n",
      " 'EHI' 'EHJ' 'EIJ' 'FGH' 'FGI' 'FGJ' 'FHI' 'FHJ' 'FIJ' 'GHI' 'GHJ' 'GIJ'\n",
      " 'HIJ']\n",
      "[3.4   6.6   5.8   5.8   4.2   1.    6.6   9.    3.4   5.8   0.9   2.74\n",
      " 2.74  0.66  0.34  2.82  3.06  2.5   2.74  3.06  3.06  3.54  0.66  3.78\n",
      " 5.94  0.9   3.06  4.9   0.9   0.58  3.06  5.22  2.74  4.9   0.9   0.58\n",
      " 3.06  5.22  2.74  4.9   0.42  3.54  3.78  0.66  0.9   0.66  0.9   0.34\n",
      " 0.58  5.94  2.82  3.06  3.06  5.22  2.74  0.522 0.522 0.378 0.594 0.81\n",
      " 0.306 0.522 2.434 0.306 0.274 2.25  2.466 2.218 2.434 0.306 0.274 2.25\n",
      " 2.466 2.218 2.434 0.57  0.594 0.282 0.306 0.282 0.306 0.274 2.538 2.226\n",
      " 2.25  2.25  2.466 2.218 2.466 0.594 0.306 0.81  2.754 0.522 2.466 0.594\n",
      " 0.306 0.81  2.754 0.522 2.466 0.354 2.97  3.186 0.378 0.594 0.378 0.594\n",
      " 0.306 3.402 0.594 0.81  0.81  2.754 0.522 0.522 0.49  2.466 4.41  2.434\n",
      " 4.378 0.594 0.81  0.306 0.522 0.306 0.522 0.274 0.49  2.754 2.25  2.466\n",
      " 2.466 4.41  2.434 0.594 0.81  0.306 0.522 0.306 0.522 0.274 0.49  2.754\n",
      " 2.25  2.466 2.466 4.41  2.434 0.354 0.378 3.186 0.57  0.594 0.594 0.81\n",
      " 0.306 0.594 0.282 0.306 0.306 0.522 0.274 2.538 2.754 2.25  2.466]\n"
     ]
    }
   ],
   "source": [
    "min_support = 0.25\n",
    "\n",
    "mask_frequent, mask_f_support = MASK(D_dataset, distortion_probability, min_support, True)\n",
    "print(mask_frequent)\n",
    "print(mask_f_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1u9UonpS2kq"
   },
   "source": [
    "#### Apriori Algorithm from mlxtend.frequent_patterns package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "c5KyN849S2kq",
    "outputId": "d0c6c941-8157-46c4-dede-985aa5433b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9\n",
      "0  1  0  1  1  0  1  0  0  1  1\n",
      "1  0  1  0  0  1  1  0  0  0  0\n",
      "2  1  0  0  0  1  1  1  0  1  0\n",
      "3  1  0  1  1  0  1  0  0  1  1\n",
      "4  0  1  0  0  1  1  0  0  0  0\n",
      "5  1  0  0  0  1  1  1  0  1  0\n",
      "6  1  0  1  1  0  1  0  0  1  1\n",
      "7  0  1  0  0  1  1  0  0  0  0\n",
      "8  1  0  0  0  1  1  1  0  1  0\n",
      "9  1  0  1  1  0  1  0  0  1  1\n",
      "    support            itemsets\n",
      "0       0.7                 (0)\n",
      "1       0.3                 (1)\n",
      "2       0.4                 (2)\n",
      "3       0.4                 (3)\n",
      "4       0.6                 (4)\n",
      "..      ...                 ...\n",
      "86      0.4     (0, 2, 5, 8, 9)\n",
      "87      0.4     (0, 3, 5, 8, 9)\n",
      "88      0.3     (0, 4, 5, 6, 8)\n",
      "89      0.4     (2, 3, 5, 8, 9)\n",
      "90      0.4  (0, 2, 3, 5, 8, 9)\n",
      "\n",
      "[91 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "T_dataset_df = pd.DataFrame(T_dataset) # providing the data in the required format for the Apriori algorithm\n",
    "print(T_dataset_df)\n",
    "apriori_frequent_itemsets = apriori(T_dataset_df, min_support = 0.25, use_colnames = True)\n",
    "print(apriori_frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "hdk1mMbXS2kq",
    "outputId": "834ef1d3-d396-4ab0-f337-2a001128619e"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def alphabetical_transform(dataset, apriori_frequent_itemsets):\n",
    "    # Generate the required letters from the alphabet\n",
    "    alphabet = string.ascii_uppercase[:dataset.shape[1]]\n",
    "\n",
    "    # Create a mapping between column index and corresponding letter\n",
    "    number_items_map = {index: letter for index, letter in enumerate(alphabet)}\n",
    "\n",
    "    # Transform the itemsets using the mapping and extract as NumPy array\n",
    "    transformed_itemsets = apriori_frequent_itemsets[\"itemsets\"].apply(\n",
    "                                lambda itemset: ''.join(number_items_map[index] for index in itemset)\n",
    "                            ).values\n",
    "\n",
    "    # Extract the support values as NumPy array\n",
    "    support_values = apriori_frequent_itemsets[\"support\"].values\n",
    "\n",
    "    return transformed_itemsets, support_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "GE_QozJ6S2kq",
    "outputId": "382c1288-fae8-4381-9b58-027f1c853353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 0 1 0]\n",
      " [1 0 1 1 0 1 0 0 1 1]]\n",
      "    support            itemsets\n",
      "0       0.7                 (0)\n",
      "1       0.3                 (1)\n",
      "2       0.4                 (2)\n",
      "3       0.4                 (3)\n",
      "4       0.6                 (4)\n",
      "..      ...                 ...\n",
      "86      0.4     (0, 2, 5, 8, 9)\n",
      "87      0.4     (0, 3, 5, 8, 9)\n",
      "88      0.3     (0, 4, 5, 6, 8)\n",
      "89      0.4     (2, 3, 5, 8, 9)\n",
      "90      0.4  (0, 2, 3, 5, 8, 9)\n",
      "\n",
      "[91 rows x 2 columns]\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'I' 'J' 'AC' 'AD' 'AE' 'AF' 'AG' 'AI' 'AJ'\n",
      " 'BE' 'BF' 'CD' 'CF' 'IC' 'JC' 'DF' 'ID' 'JD' 'EF' 'EG' 'IE' 'FG' 'IF'\n",
      " 'JF' 'IG' 'IJ' 'ACD' 'ACF' 'AIC' 'AJC' 'ADF' 'AID' 'AJD' 'AEF' 'AEG'\n",
      " 'AIE' 'AFG' 'AIF' 'AJF' 'AIG' 'AIJ' 'BEF' 'CDF' 'ICD' 'JCD' 'ICF' 'JCF'\n",
      " 'IJC' 'IDF' 'JDF' 'IJD' 'EFG' 'IEF' 'IEG' 'IFG' 'IJF' 'ACDF' 'AICD'\n",
      " 'AJCD' 'AICF' 'AJCF' 'AICJ' 'AIDF' 'AJDF' 'AIDJ' 'AEFG' 'AIEF' 'AIEG'\n",
      " 'AIFG' 'AIFJ' 'ICDF' 'JCDF' 'IJCD' 'IJCF' 'IJDF' 'IEFG' 'ACDFI' 'ACDFJ'\n",
      " 'ACDIJ' 'ACFIJ' 'ADFIJ' 'AEFGI' 'CDFIJ' 'ACDFIJ']\n",
      "[0.7 0.3 0.4 0.4 0.6 1.  0.3 0.7 0.4 0.4 0.4 0.3 0.7 0.3 0.7 0.4 0.3 0.3\n",
      " 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.6 0.3 0.3 0.3 0.7 0.4 0.3 0.4 0.4 0.4 0.4\n",
      " 0.4 0.4 0.4 0.4 0.3 0.3 0.3 0.3 0.7 0.4 0.3 0.4 0.3 0.4 0.4 0.4 0.4 0.4\n",
      " 0.4 0.4 0.4 0.4 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4\n",
      " 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.4 0.3 0.4 0.4 0.4 0.4 0.4 0.3 0.4\n",
      " 0.4]\n"
     ]
    }
   ],
   "source": [
    "print(T_dataset)\n",
    "print(apriori_frequent_itemsets)\n",
    "apriori_frequent, apriori_f_support = alphabetical_transform(T_dataset, apriori_frequent_itemsets)\n",
    "print(apriori_frequent)\n",
    "print(apriori_f_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARv3XgK6S2kr"
   },
   "source": [
    "#### Error Metrics with support_identity_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "JY_mczhSS2kr",
    "outputId": "77ee82f4-883a-4fc5-b7f4-f87202f49e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'I' 'J' 'AC' 'AD' 'AE' 'AF' 'AG' 'AI' 'AJ'\n",
      " 'BE' 'BF' 'CD' 'CF' 'IC' 'JC' 'DF' 'ID' 'JD' 'EF' 'EG' 'IE' 'FG' 'IF'\n",
      " 'JF' 'IG' 'IJ' 'ACD' 'ACF' 'AIC' 'AJC' 'ADF' 'AID' 'AJD' 'AEF' 'AEG'\n",
      " 'AIE' 'AFG' 'AIF' 'AJF' 'AIG' 'AIJ' 'BEF' 'CDF' 'ICD' 'JCD' 'ICF' 'JCF'\n",
      " 'IJC' 'IDF' 'JDF' 'IJD' 'EFG' 'IEF' 'IEG' 'IFG' 'IJF' 'ACDF' 'AICD'\n",
      " 'AJCD' 'AICF' 'AJCF' 'AICJ' 'AIDF' 'AJDF' 'AIDJ' 'AEFG' 'AIEF' 'AIEG'\n",
      " 'AIFG' 'AIFJ' 'ICDF' 'JCDF' 'IJCD' 'IJCF' 'IJDF' 'IEFG' 'ACDFI' 'ACDFJ'\n",
      " 'ACDIJ' 'ACFIJ' 'ADFIJ' 'AEFGI' 'CDFIJ' 'ACDFIJ']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'I' 'J' 'AC' 'AD' 'AE' 'AF' 'AG' 'AI' 'AJ'\n",
      " 'BE' 'BF' 'CD' 'CF' 'IC' 'JC' 'DF' 'ID' 'JD' 'EF' 'EG' 'IE' 'FG' 'IF'\n",
      " 'JF' 'IG' 'IJ' 'ACD' 'ACF' 'AIC' 'AJC' 'ADF' 'AID' 'AJD' 'AEF' 'AEG'\n",
      " 'AIE' 'AFG' 'AIF' 'AJF' 'AIG' 'AIJ' 'BEF' 'CDF' 'ICD' 'JCD' 'ICF' 'JCF'\n",
      " 'IJC' 'IDF' 'JDF' 'IJD' 'EFG' 'IEF' 'IEG' 'IFG' 'IJF' 'ACDF' 'AICD'\n",
      " 'AJCD' 'AICF' 'AJCF' 'AICJ' 'AIDF' 'AJDF' 'AIDJ' 'AEFG' 'AIEF' 'AIEG'\n",
      " 'AIFG' 'AIFJ' 'ICDF' 'JCDF' 'IJCD' 'IJCF' 'IJDF' 'IEFG' 'ACDFI' 'ACDFJ'\n",
      " 'ACDIJ' 'ACFIJ' 'ADFIJ' 'AEFGI' 'CDFIJ' 'ACDFIJ']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'I' 'J' 'AC' 'AD' 'AE' 'AF' 'AG' 'AI' 'AJ'\n",
      " 'BE' 'BF' 'CD' 'CF' 'IC' 'JC' 'DF' 'ID' 'JD' 'EF' 'EG' 'IE' 'FG' 'IF'\n",
      " 'JF' 'IG' 'IJ' 'ACD' 'ACF' 'AIC' 'AJC' 'ADF' 'AID' 'AJD' 'AEF' 'AEG'\n",
      " 'AIE' 'AFG' 'AIF' 'AJF' 'AIG' 'AIJ' 'BEF' 'CDF' 'ICD' 'JCD' 'ICF' 'JCF'\n",
      " 'IJC' 'IDF' 'JDF' 'IJD' 'EFG' 'IEF' 'IEG' 'IFG' 'IJF' 'ACDF' 'AICD'\n",
      " 'AJCD' 'AICF' 'AJCF' 'AICJ' 'AIDF' 'AJDF' 'AIDJ' 'AEFG' 'AIEF' 'AIEG'\n",
      " 'AIFG' 'AIFJ' 'ICDF' 'JCDF' 'IJCD' 'IJCF' 'IJDF' 'IEFG' 'ACDFI' 'ACDFJ'\n",
      " 'ACDIJ' 'ACFIJ' 'ADFIJ' 'AEFGI' 'CDFIJ' 'ACDFIJ']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Support Error</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11.171429</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.901099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.911132</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.824176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.263114</td>\n",
       "      <td>1.131868</td>\n",
       "      <td>0.901099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level  Support Error  False Positive  False Negative\n",
       "0      1      11.171429        0.010989        0.901099\n",
       "1      2       5.911132        0.318681        0.824176\n",
       "2      3       2.263114        1.131868        0.901099"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Levels, Support_Errors, False_Positive, False_Negative = support_identity_errors(mask_frequent, mask_f_support, apriori_frequent, apriori_f_support)\n",
    "\n",
    "data = {\n",
    "    'Level': Levels,\n",
    "    'Support Error': Support_Errors,\n",
    "    'False Positive': False_Positive,\n",
    "    'False Negative': False_Negative\n",
    "}\n",
    "\n",
    "performance = pd.DataFrame(data)\n",
    "performance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a1nK9hkS2kr"
   },
   "source": [
    "### Applying The MASK on a real dataset\n",
    "\n",
    "#### Distortion Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "M6tE6lhAS2kr",
    "outputId": "30c6f643-cd7b-4b98-b63d-415622e6886c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2      3       4     5       6       7     8      9\n",
      "0    Apple  Banana  Orange  Mango  Grapes  Eggs  Yogurt  Cheese  salt  sugar\n",
      "1        0       1       0      0       1     1       0       1     0      0\n",
      "2        0       0       0      0       0     0       0       0     0      1\n",
      "3        1       0       1      0       0     1       0       1     0      1\n",
      "4        0       0       1      1       0     1       0       0     0      1\n",
      "..     ...     ...     ...    ...     ...   ...     ...     ...   ...    ...\n",
      "995      0       1       0      0       0     0       1       0     0      0\n",
      "996      1       0       0      0       1     0       0       0     1      1\n",
      "997      1       0       0      0       1     1       0       0     0      0\n",
      "998      0       0       1      1       1     0       1       1     1      0\n",
      "999      0       0       0      0       0     0       0       0     0      1\n",
      "\n",
      "[1000 rows x 10 columns]\n",
      "[[0 1 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 0 1 ... 1 0 1]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "The distorted dataset:  [[1 0 1 ... 0 1 1]\n",
      " [1 1 1 ... 1 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 1 1 ... 1 1 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " [1 0 1 ... 1 1 0]]\n",
      "Privacy attained:  71.86866799534961 %\n"
     ]
    }
   ],
   "source": [
    "# Preparing Dataset\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = []\n",
    "with open('market_basket2.csv', 'r') as fd:\n",
    "    reader = csv.reader(fd)\n",
    "    for row in reader:\n",
    "        dataset.append(row)\n",
    "print(pd.DataFrame(dataset))\n",
    "\n",
    "# Convert the dataset it into an multidimensional array\n",
    "T_dataset = np.array(dataset)\n",
    "T_dataset = T_dataset[1:,:]\n",
    "T_dataset = T_dataset.astype(\"int\")\n",
    "print(T_dataset)\n",
    "\n",
    "# Distortion Procedure and privacy user privacy\n",
    "distortion_probability = 0.9\n",
    "s_0 = 0.01\n",
    "a = 0.75\n",
    "D_dataset = distort_dataset(T_dataset, distortion_probability)\n",
    "privacy = user_privacy(s_0, a, distortion_probability)\n",
    "print( \"The distorted dataset: \", D_dataset)\n",
    "print(\"Privacy attained: \", privacy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EelZcDjBS2ks"
   },
   "source": [
    "#### Mining Procedure\n",
    "\n",
    "MASK() will provide us with\n",
    "1. mask_frequent which is a list of all frequent itemsets and\n",
    "2. mask_f_support which is a list of their corresponding estimated supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "_u9L2SBFS2ks",
    "outputId": "560beca0-5bbc-41bf-aa97-5feaa5bd3722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PASS:  1 (  1 -itemsets)\n",
      "\n",
      "PASS:  2 (  2 -itemsets)\n",
      "\n",
      "PASS:  3 (  3 -itemsets)\n",
      "\n",
      "PASS:  4 (  4 -itemsets)\n",
      "\n",
      "PASS:  5 (  5 -itemsets)\n",
      "\n",
      "PASS:  6 (  6 -itemsets)\n",
      "\n",
      "PASS:  7 (  7 -itemsets)\n",
      "\n",
      "PASS:  8 (  8 -itemsets)\n",
      "\n",
      "PASS:  9 (  9 -itemsets)\n",
      "\n",
      "PASS:  10 (  10 -itemsets)\n",
      "The algorithm stopped because there are no more frequent itemsets!\n",
      "['A' 'B' 'C' ... 'ACDEFGHIJ' 'BCDEFGHIJ' 'ABCDEFGHIJ']\n",
      "[423.9        427.9        451.9        ...   1.29982657   1.15598815\n",
      "   0.67288153]\n"
     ]
    }
   ],
   "source": [
    "min_support = 0.25\n",
    "mask_frequent, mask_f_support = MASK(D_dataset, distortion_probability, min_support)\n",
    "print(mask_frequent)\n",
    "print(mask_f_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEznQb9JS2ks"
   },
   "source": [
    "#### Apriori Algorithm from mlxtend.frequent_patterns package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "3_JvXLIES2ks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1  2  3  4  5  6  7  8  9\n",
      "0    0  1  0  0  1  1  0  1  0  0\n",
      "1    0  0  0  0  0  0  0  0  0  1\n",
      "2    1  0  1  0  0  1  0  1  0  1\n",
      "3    0  0  1  1  0  1  0  0  0  1\n",
      "4    1  1  0  0  0  0  0  0  0  0\n",
      "..  .. .. .. .. .. .. .. .. .. ..\n",
      "994  0  1  0  0  0  0  1  0  0  0\n",
      "995  1  0  0  0  1  0  0  0  1  1\n",
      "996  1  0  0  0  1  1  0  0  0  0\n",
      "997  0  0  1  1  1  0  1  1  1  0\n",
      "998  0  0  0  0  0  0  0  0  0  1\n",
      "\n",
      "[999 rows x 10 columns]\n",
      "    support itemsets\n",
      "0  0.383383      (0)\n",
      "1  0.384384      (1)\n",
      "2  0.420420      (2)\n",
      "3  0.404404      (3)\n",
      "4  0.407407      (4)\n",
      "5  0.398398      (5)\n",
      "6  0.384384      (6)\n",
      "7  0.410410      (7)\n",
      "8  0.408408      (8)\n",
      "9  0.405405      (9)\n"
     ]
    }
   ],
   "source": [
    "T_dataset_df = pd.DataFrame(T_dataset)\n",
    "print(T_dataset_df)\n",
    "apriori_frequent_itemsets = apriori(T_dataset_df, min_support = 0.25, use_colnames = True)\n",
    "print(apriori_frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "aMKCFKz0S2ks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 0 1 ... 1 0 1]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "    support itemsets\n",
      "0  0.383383      (0)\n",
      "1  0.384384      (1)\n",
      "2  0.420420      (2)\n",
      "3  0.404404      (3)\n",
      "4  0.407407      (4)\n",
      "5  0.398398      (5)\n",
      "6  0.384384      (6)\n",
      "7  0.410410      (7)\n",
      "8  0.408408      (8)\n",
      "9  0.405405      (9)\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "[0.38338338 0.38438438 0.42042042 0.4044044  0.40740741 0.3983984\n",
      " 0.38438438 0.41041041 0.40840841 0.40540541]\n"
     ]
    }
   ],
   "source": [
    "print(T_dataset)\n",
    "print(apriori_frequent_itemsets)\n",
    "apriori_frequent, apriori_f_support = alphabetical_transform(T_dataset, apriori_frequent_itemsets)\n",
    "print(apriori_frequent)\n",
    "print(apriori_f_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-0-X_FUS2ks"
   },
   "source": [
    "#### Error Metrics with support_identity_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "z5tBcTdSS2kt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Support Error</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1093.593620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>108.338631</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18.789662</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.167540</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.150676</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Level  Support Error  False Positive  False Negative\n",
       "0      1    1093.593620             0.0             0.0\n",
       "1      2     108.338631             4.5             1.0\n",
       "2      3      18.789662            12.0             1.0\n",
       "3      4       5.167540            21.0             1.0\n",
       "4      5       2.150676            25.2             1.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(apriori_frequent)\n",
    "Levels, Support_Errors, False_Positive, False_Negative = support_identity_errors(mask_frequent, mask_f_support, apriori_frequent, apriori_f_support)\n",
    "\n",
    "data = {\n",
    "    'Level': Levels,\n",
    "    'Support Error': Support_Errors,\n",
    "    'False Positive': False_Positive,\n",
    "    'False Negative': False_Negative\n",
    "}\n",
    "\n",
    "performance = pd.DataFrame(data)\n",
    "performance.head()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "c89b155aa816127181ee565eeb7ea0c958e5b2900f0af0c2a628c340955d3678"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
