{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintaining Data Privacy in Association Rule Mining using MASK algorithm \n",
    "\n",
    "By\n",
    "\n",
    "Kozy-Korpesh Tolep, matricola: 5302354  \n",
    "\n",
    "Salah Ismail , matricola: 5239380\n",
    "\n",
    "Amir Mashmool, matricola: 5245307"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This paper discusses the challenge of obtaining accurate input data for data mining services while addressing privacy concerns. It explores whether users can be motivated to provide correct information by guaranteeing privacy protection during the mining process. proposing a scheme that distorts user data probabilistically, ensuring a high level of privacy while maintaining accurate mining results. It is this distorted information that is eventually supplied to the data miner, along with a description of the distortion procedure. The performance of the scheme is validated using real and synthetic datasets. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "The paper assumes a database model where each customer's data is represented as a tuple consisting of a fixed-length sequence of 1's and 0's. This model is commonly used for market-basket databases, where columns represent items sold by a supermarket, and each row represents a customer's purchases (1 indicating a purchase and 0 indicating no purchase).\n",
    "\n",
    "The assumption is that the number of 1's (purchases) in the database is significantly smaller than the number of 0's (non-purchases) ==> In short, the database is modeled as a large disk-resident two-dimensional sparse boolean matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining Objectives\n",
    "\n",
    "The mining objective is to efficiently discover all frequent itemsets in the database, which correspond to statistically significant and strong association rules. These rules have a support factor indicating their frequency and a confidence factor representing their strength. The goal is to find interesting rules that surpass user-defined thresholds for support and confidence. A rule is said to be “interesting” if its support and confidence are greater than user-defined thresh-olds $sup_{min}$ and $con_{min}$, respectively.\n",
    "\n",
    "example of a association rule:\n",
    "\n",
    "Transaction 1: {Bread, Milk}\n",
    "Transaction 2: {Bread, Milk, Diapers}\n",
    "\n",
    "Using these measures, we can discover association rules:\n",
    "\n",
    "\n",
    "Bread → Milk (Support: 40%, Confidence: 66.6%)\n",
    "   This rule indicates that if a customer buys bread, they are likely to buy milk as well.\n",
    "\n",
    "   \n",
    "Bread, Milk → Diapers (Support: 40%, Confidence: 50%)\n",
    "    This rule states that if a customer buys both bread and milk, they are likely to buy diapers as well.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The privacy metric\n",
    "the mechanism adopted in this paper for achieving privacy is to distort the user data before  it is subject to the mining process.\n",
    "As stated in the paper the metric is “with what probability can a given 1 or 0 in the true matrix be reconstructed”.\n",
    "It measures the probability of reconstructing distorted user data. It focuses on individual entries within customer tuples. For many applications, customers may prioritize more privacy for their 1's (purchased actions) compared to their 0's (non-purchased options)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying MASK’s Privacy\n",
    "\n",
    "In this section, we present the distortion procedure used by the MASK scheme and quantify the privacy provided by the procedure, as per the above privacy metric.\n",
    "\n",
    "### Distortion Procedure\n",
    "\n",
    "A customer tuple can be considered to be a random vector $ X = \\{X_i\\}$ , such that $X_i = 0 \\ or \\ 1$ .\n",
    "We generate the distorted vector from this customer tuple by computing $Y = distort(X)$ where $Y_i=X_i \\ XOR \\ \\overline r_i$ and $\\overline r_i$ is the complement of $r_i$, a random variable with a density function $f(r) = bernoulli(p) \\ (0 \\leq p \\leq 1)$. That is, $r_i$ takes a value 1 with probability $p$ and 0 with probability $1 - p$.\n",
    "\n",
    "The net effect of the above computation is that the identity of the $i^{th}$ element in $X$ is kept the same with probability $p$ and is flipped with probability $(1 — p)$. All the customer tuples are distorted in this fashion and make up the database supplied to the miner in effect, the miner receives a $probabilistic \\ function$ of the true customer database.\n",
    "\n",
    "### Reconstruction Probability of a 1\n",
    "${R}_1(p)$ is the probability with which a ‘1’ can be reconstructed from the distorted entry as follow:\n",
    "\n",
    "$\\mathcal{R}_1(p)=\\frac{s_0 \\times p^2}{s_0 \\times p+\\left(1-s_0\\right) \\times(1-p)}+\\frac{s_0 \\times(1-p)^2}{s_0 \\times(1-p)+\\left(1-s_0\\right) \\times p}$\n",
    "\n",
    "while $s_0$ is the average support of an item in the database. \n",
    "\n",
    "### The General Reconstruction Equation\n",
    "the relationship between $p$ and the reconstruction probability for the general case where the customer may wish to protect both his $1’s$ and $0’s$, but his concern to keep the $1’s$ private is more than that for the $0’s$.\n",
    "\n",
    "${R}_0(p)$ is the probability with which a ‘0’ can be reconstructed from the distorted entry as follow:\n",
    "\n",
    "$\\mathcal{R}_0(\\mathrm{p})=\\frac{\\left(1-s_0\\right) \\times p^2}{\\left(1-s_0\\right) \\times p+s_0 \\times(1-p)}+\\frac{\\left(1-s_0\\right) \\times(1-p)^2}{s_0 \\times p+\\left(1-s_0\\right) \\times(1-p)}$\n",
    "\n",
    "Our aim is to minimize a weighted average of ${R}_1(p)$ and ${R}_0(p)$. This corresponds to minimizing the probability of reconstruction of both $1’s$ and $0’s$. The $\\textbf {total reconstruction probability}$, ${R}(p)$, is then given as:\n",
    "\n",
    "$\\mathcal{R}(p)=a \\mathcal{R}_1(p)+(1-a) \\mathcal{R}_0(p)$\n",
    "\n",
    "where $a$ is the weight given to $1’s$ over $0’s$.\n",
    "\n",
    "\n",
    "## Privacy Measure\n",
    "Armed with the ability to compute the total reconstruction probability $\\mathcal{R}(p)$, we now simply define $\\textbf {user privacy}$ ${P}(p)$ as the following percentage:\n",
    "\n",
    "$\\mathcal{P}(p)=(1-\\mathcal{R}(p)) * 100$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distortion_probability = the distortion probability value given to distortion function\n",
    "\n",
    "distort_dataset() = a function used to distort the dataset based on distortion_probability\n",
    "\n",
    "T = the original true matrix dataset\n",
    "\n",
    "D = the distorted matrix dataset obtained with a distortion probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "\n",
    "def distort_dataset(T, distortion_probability):\n",
    "    mask = np.random.random(T.shape) > (1 - distortion_probability)\n",
    "    D = (T + mask) % 2 # %2 is to ensures that all values are either 0 or 1.\n",
    "    return D\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s_0$ is the average support of an item in the database.\n",
    "\n",
    "$a$ is the weight given to $1’s$ over $0’s$.\n",
    "\n",
    "${P}(p)$ is user privacy metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing ${R}_1(p)$ and ${R}_0(p)$\n",
    "###### Computing the total reconstruction probability ${R}(p)$\n",
    "###### Computing the privacy metric ${P}(p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_privacy(s_0, a, p):\n",
    "    r1_p = ((s_0 * (p ** 2)) / ((s_0 * p) + ((1 - s_0) * (1 - p)))) + ((s_0 * ((1 - p) ** 2)) / ((s_0 * (1 - p)) + ((1 - s_0) * p)))\n",
    "    r0_p = (((1 - s_0) * (p ** 2)) / (((1 - s_0) * p) + (s_0 * (1 - p)))) + ((s_0 * ((1 - p) ** 2)) / ((s_0 * p) + ((1 - s_0) * (1 - p))))\n",
    "    \n",
    "    r_p = (a * r1_p) + ((1 - a) * r0_p)\n",
    "    \n",
    "    p_p = (1 - r_p) * 100\n",
    "    \n",
    "    return p_p\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining the Distorted Database (MASK Algorithm)\n",
    "\n",
    "MASK’s technique is for estimating the true (accurate) supports of itemsets from a distorted database. \n",
    "We first show how to estimate the supports of $1-itemsets$ (i.e. singletons) and then present the general $n-itemset$ support estimation procedure.\n",
    "\n",
    "It is important to keep in mind that the miner is provided with both the distorted matrix as well as the distortion probability, that is, it knows the value of $p$ that was used in distorting the true matrix.\n",
    "\n",
    "### Estimating $1-itemsets$ Supports\n",
    "We denote the original true matrix by $T$ and the distorted matrix, obtained with a distortion probability of $p$, as $D$.\n",
    "\n",
    "Now consider a random individual item $i$. Let $c_1^T$ and $c_0^T$ represent the number of $1’s$ and $0’s$, respectively, in the $i$ column of $T$, while $c_1^D$ and $c_0^D$ represent the number of $1’s$ and $0’s$, respectively, in the $i$ column of $D$. With this notation, we estimate the support of $i$ in $T$ using the following equation:\n",
    "\n",
    "$\\mathbf{C}^T=\\mathbf{M}^{-1} \\mathbf{C}^{\\mathrm{D}}$\n",
    "\n",
    "Where\n",
    "\n",
    "$M=\\left[\\begin{array}{cc}p & 1-p \\\\ 1-p & p\\end{array}\\right] \\  C^D=\\left[\\begin{array}{c}c_1^D \\\\ c_0^D\\end{array}\\right] C^T=\\left[\\begin{array}{c}c_1^T \\\\ c_0^T\\end{array}\\right]$\n",
    "\n",
    "The M matrix in the above equation incorporates the observation that by our method of distortion, if a column had $n \\ 1’s$ in $T$, these $1’s$ will generate approximately $pn \\ 1’s$ and $(1 — p)n \\ 0’s$ for the same column in $D$. Similarly for the $0’s$ of this column in $T$. Therefore, given $c_1^D$ and $c_0^D$, it is possible to estimate the value of $c_1^T$, that is, the true support of item $i$.\n",
    "\n",
    "\n",
    "### Estimating $n-itemsets$ Supports\n",
    "To compute the support for an arbitrary n-itemset. For this general case, we define the matrices as:\n",
    "\n",
    "$C^D=\\left[\\begin{array}{c}c_{2^n-1}^D \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ c_1^D \\\\ c_0^D\\end{array}\\right] \\quad \\ C^T=\\left[\\begin{array}{c}c_{2^n-1}^T \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ c_1^T \\\\ c_0^T\\end{array}\\right]$\n",
    "\n",
    "Here $c_k^T$ should be interpreted as the count of the tuples in $T$ that have the binary form of $k$ (in $n$ digits) for the given itemset (that is, for a 2 -itemset, $c_2^T$ refers to the count of 10's in the columns of $T$ corresponding to that itemset, $c_3^T$ to the count of 11 's, and so on). Similarly, $c_k^D$ is defined for the distorted matrix $D$. Finally, the matrix $\\mathbf{M}$ is defined as:\n",
    "\n",
    "$m_{i, j}=$ The probability that a tuple of the form corresponding to $c_j^T$ in $T$ goes to a tuple of the form corresponding to $c_i^D$ in $D$.\n",
    "\n",
    "For example, $m_{1, 2}$ for a $2-itemset$ is the probability that a $10$ tuple distorts to a $01$ tuple. Accordingly, $m_{1, 2}=$ $(1-p)(1-p)$. The basis for this formulation lies in the fact that in our distortion procedure, the component columns of an $n-itemset$ are distorted $idependently$. Therefore, we can use the product of the probability terms.\n",
    "\n",
    "### The Full Mining Process\n",
    "The above equations help us to estimate the value of $c_{2^n-1}^T$ for an $n-itemset$ by using the values of $c_i^D$, $0 \\leq i \\leq 2^n - 1$. But, first we need to compute the $c_i^D$ values themselves. For this purpose, after the modifications described below, we have currently implemented our system based on the classical Apriori algorithm.\n",
    "\n",
    "The Apriori algorithm is a multi-pass algorithm that discovers frequent itemsets by incrementally increasing the length of itemsets. It uses the AprioriGen algorithm to generate candidate itemsets for each pass based on the frequent itemsets found in the previous pass. This approach helps efficiently search for frequent itemsets in large datasets by focusing on subsets that are likely to be frequent.\n",
    "\n",
    "In our approach the \"MASK\", unlike the Apriori algorithm, we consider and keep track of all possible combinations when counting 2-itemsets. Apriori only counts the occurrences of '11' in the tuples for each candidate 2-itemset, indicating both items appearing together. However, our approach requires tracking the counts for all combinations: '00', '01', '10', and '11'. This difference in counting allows us to capture more detailed information about itemset co-occurrences in the data.\n",
    "\n",
    "## MASK Mining Optimizations\n",
    "\n",
    "### Linear Number of Counters\n",
    "### Reducing Amount of Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the MASK() here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Framework\n",
    "Due to the probabilistic nature of MASK, the reconstructed support values are not expected to match the actual supports exactly. This can lead to errors where the reported support values are either larger or smaller than the actual supports.\n",
    "\n",
    "Errors in support estimation can have a significant impact, the probabilistic evaluation of supports may incorrectly classify \"border-line\" itemsets as either frequent or rare. This results in both false positives (itemsets wrongly reported as frequent) and false negatives (itemsets wrongly reported as rare).\n",
    "\n",
    "We evaluate the mining process under two conditions. The first condition uses the $sup_{min}$ value provided by the user, while the second condition uses a slightly lower $sup_{min}$ value.\n",
    "\n",
    "We prioritize coverage over precision in this evaluation. Specifically, we consider a 10% reduction in the $sup_{min}$ value (r = 10%).\n",
    "\n",
    "To quantify the errors made, We compare the mining outputs obtained from MASK with those derived from Apriori running on the true (undistorted) database. We compare the results using both the $sup_{min}$ value provided by the user and the lowered $sup_{min}$ value. This allows to evaluate the impact of the lowered $sup_{min}$ value on the mining process and assess the differences between MASK and Apriori.\n",
    "\n",
    "### Error Metrics\n",
    "\n",
    "1.Support Error  $\\rho$.\n",
    "\n",
    "This metric reflects the (percentage) average relative error in the reconstructed support values for those itemsets that are correctly  identified to be frequent. Denoting the reconstructed support by rec_sup and the actual support by act_sup, the support error is computed over all frequent itemsets as\n",
    "\n",
    "$\\rho=\\frac{1}{|f|} \\Sigma_f \\frac{\\left|r e c_{-} s u p_f-a c t_{-} s u p_f\\right|}{a c t_{\\_} s u p_f} * 100$\n",
    "\n",
    "2.Identity Error.\n",
    "\n",
    "This metric reflects the (percentage) error in identifying frequent itemsets and has two components: $\\sigma^{+}$, indicating the percentage of false positives, and $\\sigma^{-}$ indicating the percentage of false negatives. Denoting the reconstructed set of frequent itemsets with $R$ and the correct set of frequent itemsets with $F$, these metrics are computed as:\n",
    "\n",
    "$\\sigma^{+}=\\frac{|R-F|}{|F|} * 100 \\quad \\sigma^{-}=\\frac{|F-R|}{|F|} * 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_support_error(mask_f, mask_f_s, apriori_f_s):\n",
    "    \"\"\"\n",
    "    Calculates the support error metric.\n",
    "\n",
    "    Args:\n",
    "    mask_f (list): List of frequent itemsets obtained from MASK.\n",
    "    mask_f_s (list): List of estimated supports of frequent itemsets from MASK.\n",
    "    apriori_f_s (list): List of actual supports of frequent itemsets from Apriori.\n",
    "\n",
    "    Returns:\n",
    "    float: Support error.\n",
    "    \"\"\"\n",
    "    total_itemsets = len(mask_f)\n",
    "    \n",
    "    # Calculate the relative support error for each itemset and sum them up\n",
    "    relative_errors = [abs(rec_sup - act_sup) / act_sup for rec_sup, act_sup in zip(mask_f_s, apriori_f_s)]\n",
    "    sum_of_errors = sum(relative_errors)\n",
    "\n",
    "    # Calculate the average relative support error\n",
    "    average_error = sum_of_errors / total_itemsets\n",
    "\n",
    "    support_error = average_error\n",
    "    return support_error\n",
    "\n",
    "def calculate_identity_error(mask_f, apriori_f):\n",
    "    \"\"\"\n",
    "    Calculates the identity error metrics.\n",
    "\n",
    "    Args:\n",
    "    mask_f (list): List of frequent itemsets obtained from MASK.\n",
    "    apriori_f (list): List of frequent itemsets obtained from Apriori.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Identity error (false positives, false negatives).\n",
    "    \"\"\"\n",
    "    reconstructed_itemsets = set(mask_f)\n",
    "    actual_itemsets = set(apriori_f)\n",
    "    false_positives = len(reconstructed_itemsets - actual_itemsets) / len(actual_itemsets)\n",
    "    false_negatives = len(actual_itemsets - reconstructed_itemsets) / len(actual_itemsets)\n",
    "    return false_positives, false_negatives\n",
    "\n",
    "def support_identity_errors(mask_f, mask_f_s, apriori_f, apriori_f_s):\n",
    "    nbr_m_f = np.size(mask_f)\n",
    "    max_levels = len(mask_f[nbr_m_f-1])\n",
    "\n",
    "    levels = []\n",
    "    support_errors = []\n",
    "    false_positive_errors = []\n",
    "    false_negative_errors = []\n",
    "\n",
    "    for l in range(max_levels):\n",
    "        level = l+1\n",
    "        level_mask_f = []\n",
    "        level_mask_f_s = []\n",
    "        for i in range(np.size(mask_f)):\n",
    "            if len(mask_f[i]) == level:\n",
    "                level_mask_f.append(mask_f[i])\n",
    "                level_mask_f_s.append(mask_f_s[i])\n",
    "\n",
    "        support_error = calculate_support_error(level_mask_f, level_mask_f_s, apriori_f_s)\n",
    "\n",
    "        false_positive, false_negative = calculate_identity_error(level_mask_f, apriori_f)\n",
    "\n",
    "        levels.append(level)\n",
    "        support_errors.append(support_error)\n",
    "        false_positive_errors.append(false_positive)\n",
    "        false_negative_errors.append(false_negative)\n",
    "\n",
    "    return levels, support_errors, false_positive_errors, false_negative_errors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Applying The MASK on a synthetic dataset\n",
    "\n",
    "#### Distortion Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating True matrix T_dataset\n",
    "T_dataset = np.array([\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
    "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "[1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "])\n",
    "\n",
    "distortion_probability = 0.7\n",
    "\n",
    "# building the distorted matrix D_dataset by distorting T_dataset\n",
    "D_dataset = distort_dataset(T_dataset, distortion_probability)\n",
    "s_0 = 0.01\n",
    "a = 0.75\n",
    "privacy = user_privacy(s_0, a, distortion_probability)\n",
    "print(\"True matrix: \", T_dataset)\n",
    "print(\"Distorted matrix: \", D_dataset)\n",
    "print(\"User Privacy attained: \", privacy, \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mining Procedure\n",
    "\n",
    "MASK() will provide us with\n",
    "1. mask_frequent which is a list of all frequent itemsets and \n",
    "2. mask_support which is a list of their corresponding estimated supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.25\n",
    "mask_frequent, mask_support = MASK(D_dataset, distortion_probability, min_support)\n",
    "print(mask_frequent)\n",
    "print(mask_support)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori Algorithm from mlxtend.frequent_patterns package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salah.ismail\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The allowed values for a DataFrame are True, False, 0, 1. Found value 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmlxtend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfrequent_patterns\u001b[39;00m \u001b[39mimport\u001b[39;00m apriori, association_rules \n\u001b[0;32m      4\u001b[0m TrueDataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(T_dataset) \u001b[39m# providing the data in the required format for the Apriori algorithm \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m apriori_frequent_itemsets \u001b[39m=\u001b[39m apriori(TrueDataset, min_support \u001b[39m=\u001b[39;49m \u001b[39m0.25\u001b[39;49m, use_colnames \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(apriori_frequent_itemsets)\n",
      "File \u001b[1;32mc:\\Users\\salah.ismail\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\apriori.py:241\u001b[0m, in \u001b[0;36mapriori\u001b[1;34m(df, min_support, use_colnames, max_len, verbose, low_memory)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m min_support \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`min_support` must be a positive \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnumber within the interval `(0, 1]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m min_support\n\u001b[0;32m    239\u001b[0m     )\n\u001b[1;32m--> 241\u001b[0m fpc\u001b[39m.\u001b[39;49mvalid_input_check(df)\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(df, \u001b[39m\"\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    244\u001b[0m     \u001b[39m# DataFrame with SparseArray (pandas >= 0.24)\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39mif\u001b[39;00m df\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\salah.ismail\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:132\u001b[0m, in \u001b[0;36mvalid_input_check\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    127\u001b[0m val \u001b[39m=\u001b[39m values[\u001b[39mtuple\u001b[39m(loc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m idxs)]\n\u001b[0;32m    128\u001b[0m s \u001b[39m=\u001b[39m (\n\u001b[0;32m    129\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe allowed values for a DataFrame\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m are True, False, 0, 1. Found value \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (val)\n\u001b[0;32m    131\u001b[0m )\n\u001b[1;32m--> 132\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(s)\n",
      "\u001b[1;31mValueError\u001b[0m: The allowed values for a DataFrame are True, False, 0, 1. Found value 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules \n",
    "\n",
    "TrueDataset = pd.DataFrame(T_dataset) # providing the data in the required format for the Apriori algorithm \n",
    "apriori_frequent_itemsets = apriori(TrueDataset, min_support = 0.25, use_colnames = True) \n",
    "print(apriori_frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def alphabetical_transform(dataset, apriori_frequent_itemsets):\n",
    "    # Generate the required letters from the alphabet\n",
    "    alphabet = string.ascii_uppercase[:dataset.shape[1]]\n",
    "\n",
    "    # Create a mapping between column index and corresponding letter\n",
    "    number_items_map = {index: letter for index, letter in enumerate(alphabet)}\n",
    "\n",
    "    # Transform the itemsets using the mapping and extract as NumPy array\n",
    "    transformed_itemsets = apriori_frequent_itemsets[\"itemsets\"].apply(\n",
    "                                lambda itemset: ''.join(number_items_map[index] for index in itemset)\n",
    "                            ).values\n",
    "\n",
    "    # Extract the support values as NumPy array\n",
    "    support_values = apriori_frequent_itemsets[\"support\"].values\n",
    "\n",
    "    return transformed_itemsets, support_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_f, apriori_f_s = alphabetical_transform(T_dataset, apriori_frequent_itemsets)\n",
    "print(apriori_f)\n",
    "print(apriori_f_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Metrics with support_identity_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levels, Support_Errors, False_Positive, False_Negative = support_identity_errors(mask_f, mask_f_s,apriori_f, apriori_f_s)\n",
    "\n",
    "data = {\n",
    "    'Level': Levels,\n",
    "    'Support Error': Support_Errors,\n",
    "    'False Positive': False_Positive,\n",
    "    'False Negative': False_Negative\n",
    "}\n",
    "\n",
    "performance = pd.DataFrame(data)\n",
    "performance.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying The MASK on a real dataset\n",
    "\n",
    "#### Distortion Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0      1       2       3      4       5     6       7    \n",
      "0     TransactionID  Apple  Banana  Orange  Mango  Grapes  Eggs  Yogurt  \\\n",
      "1                 0      1       0       0      1       0     0       0   \n",
      "2                 1      0       0       0      0       0     1       0   \n",
      "3                 2      0       0       0      1       0     0       0   \n",
      "4                 3      0       0       0      0       0     0       0   \n",
      "...             ...    ...     ...     ...    ...     ...   ...     ...   \n",
      "996             995      1       0       0      0       0     0       0   \n",
      "997             996      0       0       0      0       0     0       0   \n",
      "998             997      0       0       1      0       0     1       0   \n",
      "999             998      0       0       0      0       0     0       0   \n",
      "1000            999      1       0       0      0       0     0       0   \n",
      "\n",
      "          8     9      10  \n",
      "0     Cheese  salt  sugar  \n",
      "1          0     0      0  \n",
      "2          0     0      0  \n",
      "3          0     0      1  \n",
      "4          0     0      0  \n",
      "...      ...   ...    ...  \n",
      "996        0     0      0  \n",
      "997        0     0      0  \n",
      "998        0     0      0  \n",
      "999        0     1      0  \n",
      "1000       1     0      0  \n",
      "\n",
      "[1001 rows x 11 columns]\n",
      "[[  0   1   0 ...   0   0   0]\n",
      " [  1   0   0 ...   0   0   0]\n",
      " [  2   0   0 ...   0   0   1]\n",
      " ...\n",
      " [997   0   0 ...   0   0   0]\n",
      " [998   0   0 ...   0   1   0]\n",
      " [999   1   0 ...   1   0   0]]\n",
      "mask [[ True  True  True ... False  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " [False False  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True False ...  True  True  True]\n",
      " [False  True  True ...  True  True False]\n",
      " [ True  True  True ...  True  True False]]\n",
      "The distorted dataset:  [[1 0 1 ... 0 1 1]\n",
      " [0 0 1 ... 1 1 1]\n",
      " [0 0 1 ... 1 1 0]\n",
      " ...\n",
      " [0 1 0 ... 1 1 1]\n",
      " [0 1 1 ... 1 0 0]\n",
      " [0 0 1 ... 0 1 0]]\n",
      "Privacy attained:  81.19555353901997 %\n"
     ]
    }
   ],
   "source": [
    "# Preparing Dataset\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = []\n",
    "with open('market_basket.csv', 'r') as fd:\n",
    "    reader = csv.reader(fd)\n",
    "    for row in reader:\n",
    "        dataset.append(row)\n",
    "print(pd.DataFrame(dataset))\n",
    "\n",
    "# Convert the dataset it into an multidimensional array\n",
    "T_dataset = np.array(dataset)\n",
    "T_dataset = T_dataset[1:,:]\n",
    "T_dataset = T_dataset.astype(\"int\")\n",
    "print(T_dataset)\n",
    "\n",
    "# Distortion Procedure and privacy user privacy\n",
    "distortion_probability = 0.7\n",
    "s_0 = 0.01\n",
    "a = 0.75\n",
    "D_dataset = distort_dataset(T_dataset, distortion_probability)\n",
    "privacy = user_privacy(s_0, a, distortion_probability)\n",
    "print( \"The distorted dataset: \", D_dataset)\n",
    "print(\"Privacy attained: \", privacy, \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mining Procedure\n",
    "\n",
    "MASK() will provide us with\n",
    "1. mask_frequent which is a list of all frequent itemsets and \n",
    "2. mask_support which is a list of their corresponding estimated supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.25\n",
    "mask_frequent, mask_support = MASK(D_dataset, distortion_probability, min_support)\n",
    "print(mask_frequent)\n",
    "print(mask_support)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori Algorithm from mlxtend.frequent_patterns package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueDataset = pd.DataFrame(T_dataset)\n",
    "apriori_frequent_itemsets = apriori(TrueDataset, min_support = 0.25, use_colnames = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori_f, apriori_f_s = alphabetical_transform(T_dataset, apriori_frequent_itemsets)\n",
    "print(apriori_f)\n",
    "print(apriori_f_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Metrics with support_identity_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levels, Support_Errors, False_Positive, False_Negative = support_identity_errors(mask_f, mask_f_s,apriori_f, apriori_f_s)\n",
    "\n",
    "data = {\n",
    "    'Level': Levels,\n",
    "    'Support Error': Support_Errors,\n",
    "    'False Positive': False_Positive,\n",
    "    'False Negative': False_Negative\n",
    "}\n",
    "\n",
    "performance = pd.DataFrame(data)\n",
    "performance.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
